{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.8\n",
      "numpy: 1.26.4\n",
      "pandas: 2.2.1\n",
      "scipy: 1.12.0\n",
      "scikit-learn: 1.5.1\n",
      "dill: 0.3.8\n",
      "joblib: 1.3.2\n",
      "psutil: 5.9.8\n",
      "pip: 24.0\n",
      "surmise: 0.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/ygz14_194fs1mqhvr0jwdcc80000gn/T/ipykernel_55036/3590732199.py:2: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pkg_resources\n",
    "\n",
    "# Get Python version\n",
    "python_version = sys.version.split(\" \")[0]\n",
    "\n",
    "# Define the packages\n",
    "packages = [\"numpy\", \"pandas\", \"scipy\", \"scikit-learn\", \"dill\", \"joblib\", \"psutil\", \"pip\", \"surmise\"]\n",
    "\n",
    "# Get package versions, skipping those that are not installed\n",
    "package_versions = {}\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        package_versions[pkg] = pkg_resources.get_distribution(pkg).version\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        package_versions[pkg] = \"Not installed\"\n",
    "\n",
    "print(f\"Python version: {python_version}\")\n",
    "for pkg, version in package_versions.items():\n",
    "    print(f\"{pkg}: {version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulator train and test : No k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../../surmise/emulationmethods'))\n",
    "sys.path.append(os.path.abspath('../../surmise'))\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dill\n",
    "import gzip\n",
    "\n",
    "from emulation import emulator\n",
    "from AKSGP import Emulator as emulator_AKSGP\n",
    "from PCGP_scikit import Emulator as PCGP_scikit\n",
    "\n",
    "import logging\n",
    "\n",
    "# Configure logging for the Emulator class\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Load training data\n",
    "train_dir = 'simulation_data/Pb_Pb_2760_Grad/train'\n",
    "X = np.loadtxt(os.path.join(train_dir, 'X.txt'))\n",
    "Ymean = np.loadtxt(os.path.join(train_dir, 'Ymean.txt'))\n",
    "Ystd = np.loadtxt(os.path.join(train_dir, 'Ystd.txt'))\n",
    "\n",
    "# for quick tests\n",
    "numdesignpt = 300\n",
    "numobs = 4\n",
    "X = X[:numdesignpt, :]\n",
    "Ymean = Ymean[:numdesignpt,:numobs]\n",
    "Ystd = Ystd[:numdesignpt,:numobs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xloc = np.arange(Ymean.shape[1])  # refers to the observable indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 20:53:35 - __main__ - INFO - Emulator 'PCGP' trained and saved.\n",
      "\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k2__length_scale is close to the specified upper bound 998.2049999999999. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 5 of parameter k1__k2__length_scale is close to the specified lower bound 0.671716. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 6 of parameter k1__k2__length_scale is close to the specified lower bound 0.23506800000000003. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 9 of parameter k1__k2__length_scale is close to the specified upper bound 296.815. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 10 of parameter k1__k2__length_scale is close to the specified upper bound 18.898. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 14 of parameter k1__k2__length_scale is close to the specified lower bound 0.636364. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 15 of parameter k1__k2__length_scale is close to the specified lower bound 2.3887480000000005. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 16 of parameter k1__k2__length_scale is close to the specified lower bound 0.013956000000000003. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 4 of parameter k1__k2__length_scale is close to the specified upper bound 476.046. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 6 of parameter k1__k2__length_scale is close to the specified upper bound 58.767. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 16 of parameter k1__k2__length_scale is close to the specified upper bound 3.4890000000000003. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 4 of parameter k1__k2__length_scale is close to the specified upper bound 476.046. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 8 of parameter k1__k2__length_scale is close to the specified upper bound 294.521. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 13 of parameter k1__k2__length_scale is close to the specified upper bound 12.346999999999998. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 15 of parameter k1__k2__length_scale is close to the specified upper bound 597.1870000000001. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k2__length_scale is close to the specified lower bound 3.99282. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 2 of parameter k1__k2__length_scale is close to the specified upper bound 168.102. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 3 of parameter k1__k2__length_scale is close to the specified lower bound 0.39299200000000006. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 5 of parameter k1__k2__length_scale is close to the specified upper bound 167.929. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 6 of parameter k1__k2__length_scale is close to the specified upper bound 58.767. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 7 of parameter k1__k2__length_scale is close to the specified upper bound 16.813. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 8 of parameter k1__k2__length_scale is close to the specified upper bound 294.521. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 10 of parameter k1__k2__length_scale is close to the specified upper bound 18.898. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 11 of parameter k1__k2__length_scale is close to the specified upper bound 18.862. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 12 of parameter k1__k2__length_scale is close to the specified upper bound 17.616. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 13 of parameter k1__k2__length_scale is close to the specified upper bound 12.346999999999998. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 15 of parameter k1__k2__length_scale is close to the specified upper bound 597.1870000000001. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "2024-09-09 20:53:52 - PCGP_scikit - INFO - GP 0 score : 0.9644108910971223\n",
      "2024-09-09 20:53:52 - PCGP_scikit - INFO - GP 1 score : 0.8688748887367096\n",
      "2024-09-09 20:53:52 - PCGP_scikit - INFO - GP 2 score : 0.21184906788210045\n",
      "2024-09-09 20:53:52 - PCGP_scikit - INFO - GP 3 score : 0.1795463331345818\n",
      "2024-09-09 20:53:52 - __main__ - INFO - Emulator 'PCGP_scikit' trained and saved.\n",
      "\n",
      "2024-09-09 20:54:05 - __main__ - INFO - Emulator 'PCSK' trained and saved.\n",
      "\n",
      "2024-09-09 20:54:05 - AKSGP - INFO - Automatic kernel selection opted. Best kernel for each output dimension will be selected from the list of kernels:\n",
      "   ['Matern12', 'Matern32', 'Matern52', 'RBF']\n",
      "\n",
      "2024-09-09 20:54:05 - AKSGP - INFO - Shape of training arrays: (270, 17), (270, 4), (270, 4)\n",
      "2024-09-09 20:54:05 - AKSGP - INFO - Shape of validation arrays: (30, 17), (30, 4), (30, 4)\n",
      "2024-09-09 20:54:05 - AKSGP - INFO - Training GPs with all available kernels...\n",
      "2024-09-09 20:54:05 - AKSGP - INFO -   Standardizing input space...\n",
      "2024-09-09 20:54:05 - AKSGP - INFO -   Standardizing data...\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 4 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 6 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 16 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 4 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 6 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 8 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 4 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 8 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 16 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 4 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 15 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 16 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "2024-09-09 20:54:10 - AKSGP - INFO -   Trained GPs with Matern12 kernels.\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 16 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 6 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 8 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 16 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "2024-09-09 20:54:15 - AKSGP - INFO -   Trained GPs with Matern32 kernels.\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 6 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 8 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 16 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "2024-09-09 20:54:20 - AKSGP - INFO -   Trained GPs with Matern52 kernels.\n",
      "2024-09-09 20:54:22 - AKSGP - INFO -   Trained GPs with RBF kernels.\n",
      "2024-09-09 20:54:22 - AKSGP - INFO - Finding best kernels for each output dimension...\n",
      "2024-09-09 20:54:22 - AKSGP - INFO -   Selected best kernels for each output dimension:\n",
      "   ['Matern52', 'Matern32', 'Matern32', 'Matern32']\n",
      "\n",
      "2024-09-09 20:54:22 - AKSGP - INFO - Retraining the GPs with selected best kernels using all training data...\n",
      "2024-09-09 20:54:22 - AKSGP - INFO -   Standardizing input space...\n",
      "2024-09-09 20:54:22 - AKSGP - INFO -   Standardizing data...\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 16 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/Users/jaiswal/miniconda3/envs/jssims_new/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 16 of parameter k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "2024-09-09 20:54:27 - AKSGP - INFO - Retraining GPs complete.\n",
      "\n",
      "2024-09-09 20:54:27 - AKSGP - INFO - Kernel after GP training for output dimension 0:\n",
      "2.31**2 * Matern(length_scale=[7.83, 28.4, 24.3, 20.5, 113, 7.69, 17.6, 15.7, 40.9, 7.6, 20, 14.8, 6.35, 20.1, 18.8, 30.6, 63.4], nu=2.5)\n",
      "  Log-marginal-likelihood: 56.40068746917024\n",
      "\n",
      "2024-09-09 20:54:27 - AKSGP - INFO - Kernel after GP training for output dimension 1:\n",
      "4.75**2 * Matern(length_scale=[20.3, 20.4, 114, 60.2, 170, 15.7, 43.3, 38.1, 125, 19.3, 47.4, 38.6, 15.4, 49.6, 48.7, 126, 1e+03], nu=1.5)\n",
      "  Log-marginal-likelihood: 4.388422991961704\n",
      "\n",
      "2024-09-09 20:54:27 - AKSGP - INFO - Kernel after GP training for output dimension 2:\n",
      "3**2 * Matern(length_scale=[11.8, 12.2, 59, 40, 97.7, 12, 30.8, 26, 69.5, 12.7, 26.3, 24.7, 10.6, 37.7, 34.3, 92.2, 1e+03], nu=1.5)\n",
      "  Log-marginal-likelihood: -34.78268437018323\n",
      "\n",
      "2024-09-09 20:54:27 - AKSGP - INFO - Kernel after GP training for output dimension 3:\n",
      "1.89**2 * Matern(length_scale=[7.4, 6.47, 19.6, 17, 47.6, 8.7, 370, 18.2, 53.7, 9.46, 19.8, 13.2, 9.09, 19.5, 19.8, 65.2, 24.1], nu=1.5)\n",
      "  Log-marginal-likelihood: -74.12595357067781\n",
      "\n",
      "2024-09-09 20:54:27 - __main__ - INFO - Emulator 'AKSGP' trained and saved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to train and save emulators. \n",
    "\n",
    "methods = ['PCGP', 'PCGP_scikit', 'PCSK', 'AKSGP']  # specify only the emulators you want to train\n",
    "\n",
    "emus = {}\n",
    "for method in methods:\n",
    "\n",
    "    if method == 'PCGP':\n",
    "        prior = {'min': np.min(Ymean.T), 'max': np.max(Ymean.T)}\n",
    "        args = {'prior': prior}\n",
    "        emus[method] = emulator(x=xloc, theta=X, f=Ymean.T, method=method, args=args)\n",
    "        \n",
    "    elif method == 'PCSK':\n",
    "        args = {'simsd': Ystd.T}\n",
    "        emus[method] = emulator(x=xloc, theta=X, f=Ymean.T, method=method, args=args)\n",
    "        \n",
    "    elif method == 'AKSGP':\n",
    "        emus[method] = emulator_AKSGP(X=X, Y_mean=Ymean, Y_std=Ystd)\n",
    "        emus[method].fit(kernel='AKS', nrestarts=10, seed=None)\n",
    "        \n",
    "    elif method == 'PCGP_scikit':\n",
    "        emus[method] = PCGP_scikit(X=X, Y=Ymean, npc = 10)\n",
    "        emus[method].fit(nrestarts=10)\n",
    "        \n",
    "    else:\n",
    "        logger.error(f\"Unknown method '{method}'.\\n\")\n",
    "        continue\n",
    "        \n",
    "    # Saving the emulators after training\n",
    "    try:\n",
    "        filename = f'emulator_{method}.dill.gz'\n",
    "        with gzip.open(filename, 'wb') as f:\n",
    "            dill.dump(emus[method], f)\n",
    "        logger.info(f\"Emulator '{method}' trained and saved.\\n\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save emulator '{method}': {e}\\n\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulators = {}\n",
    "for method in ['PCGP', 'PCGP_scikit', 'PCSK', 'AKSGP']:\n",
    "    filename = f'emulator_{method}.dill.gz'\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        emulators[method] = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 20:54:27 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from metrics import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load testing data\n",
    "test_dir = 'simulation_data/Pb_Pb_2760_Grad/test'\n",
    "\n",
    "X_test = np.loadtxt(os.path.join(test_dir, 'X.txt'))\n",
    "Ymean_test = np.loadtxt(os.path.join(test_dir, 'Ymean.txt'))\n",
    "Ystd_test = np.loadtxt(os.path.join(test_dir, 'Ystd.txt'))\n",
    "\n",
    "\n",
    "Ymean_test = Ymean_test[:numdesignpt,:numobs]\n",
    "Ystd_test = Ystd_test[:numdesignpt,:numobs]\n",
    "\n",
    "def metrics_cal(means1, var1, means2, var2):\n",
    "    # Initialize array's to store the distances\n",
    "    kl_div = np.zeros(means1.shape)\n",
    "    wasserstein_dist = np.zeros(means1.shape)\n",
    "    hellinger_dist = np.zeros(means1.shape)\n",
    "    \n",
    "    # Loop over each pair of means and variances\n",
    "    for i in range(means1.shape[0]):\n",
    "        for j in range(means1.shape[1]):\n",
    "            mu1 = means1[i, j]\n",
    "            mu2 = means2[i, j]\n",
    "            var1_ij = var1[i, j]\n",
    "            var2_ij = var2[i, j]\n",
    "            \n",
    "            # Calculate the distances for the current pair\n",
    "            kl_div[i, j] = kl_divergence_gaussian(mu1=mu1, Cov1=var1_ij, mu2=mu2, Cov2=var2_ij)\n",
    "            hellinger_dist[i, j] = hellinger_distance_gaussian(mu1=mu1, Cov1=var1_ij, mu2=mu2, Cov2=var2_ij)\n",
    "            wasserstein_dist[i, j] = wasserstein_distance_gaussian(mu1=mu1, Cov1=var1_ij, mu2=mu2, Cov2=var2_ij)\n",
    "\n",
    "    return kl_div, hellinger_dist, wasserstein_dist\n",
    "\n",
    "\n",
    "scaler_Y = StandardScaler()\n",
    "Ymean_test = scaler_Y.fit_transform(Ymean_test)\n",
    "Ystd_test = Ystd_test / scaler_Y.scale_\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "for method, emu in emulators.items():\n",
    "    # print(method)\n",
    "    \n",
    "    if method in {'PCGP', 'PCSK'}:\n",
    "        pred = emu.predict(x=xloc, theta=X_test)\n",
    "        predmean = pred.mean().T\n",
    "        predvar = pred.var().T\n",
    "        \n",
    "        predmean = scaler_Y.transform(predmean)\n",
    "        predvar = np.square(np.sqrt(predvar)/scaler_Y.scale_)\n",
    "        \n",
    "    elif method in {'AKSGP', 'PCGP_scikit'}:\n",
    "        predmean, predstd = emu.predict(X_test)\n",
    "        predvar = np.square(predstd)\n",
    "\n",
    "        predmean = scaler_Y.transform(predmean)\n",
    "        predvar = np.square(np.sqrt(predvar)/scaler_Y.scale_)\n",
    "    \n",
    "    Yvar_test = np.square(Ystd_test)\n",
    "    \n",
    "    EC = intervalstats(Ymean_test, predmean, predvar)\n",
    "    RMSE = rmse(Ymean_test, predmean)\n",
    "    NRMSE = normalized_rmse(Ymean_test, predmean)\n",
    "    KLdiv, HD, WD = metrics_cal(predmean, predvar, Ymean_test, Yvar_test)\n",
    "\n",
    "    # Store the results in the list\n",
    "    results.append({\n",
    "        'Method': method,\n",
    "        '95% Coverage': '{:.6f}'.format(EC[0]),\n",
    "        'PI Width': '{:.6f}'.format(EC[1]),\n",
    "        'RMSE': '{:.6f}'.format(RMSE),\n",
    "        'NRMSE': '{:.6f}'.format(NRMSE),\n",
    "        'KL Divergence': '{:.6f}'.format(np.mean(KLdiv)),\n",
    "        'Hellinger Distance': '{:.6f}'.format(np.mean(HD)),\n",
    "        'Wasserstein Distance': '{:.6f}'.format(np.mean(WD)),\n",
    "        'Training time (s)': '{:.6f}'.format(emu.trainwallclocktime),\n",
    "        'Prediction time (s)': '{:.6f}'.format(emu.predictwallclocktime),\n",
    "    })\n",
    "\n",
    "# Convert the list of results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric                        PCGP            PCGP_scikit     PCSK            AKSGP          \n",
      "---------------------------------------------------------------------------------------------\n",
      "95% Coverage                  0.908602        0.967742        0.873656        0.884409       \n",
      "PI Width                      0.727235        0.952633        0.811478        0.749150       \n",
      "RMSE                          0.215053        0.243941        0.248784        0.229805       \n",
      "NRMSE                         0.051420        0.058327        0.059494        0.055017       \n",
      "KL Divergence                 20.942500       32.780621       29.081159       24.569754      \n",
      "Hellinger Distance            0.675628        0.707023        0.696260        0.681411       \n",
      "Wasserstein Distance          0.235710        0.295790        0.270833        0.249846       \n",
      "---------------------------------------------------------------------------------------------\n",
      "Training time (s)             7.761672        16.943299       12.469728       21.782023      \n",
      "Prediction time (s)           0.010360        0.003323        0.012495        0.003018       \n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get all the methods from the dataframe\n",
    "methods = results_df['Method'].values\n",
    "\n",
    "# Create the transposed header with methods\n",
    "header = f\"{'Metric':<30}\" + \" \".join([f\"{method:<15}\" for method in methods])\n",
    "print(header)\n",
    "print('-' * len(header))\n",
    "\n",
    "# Define the metrics to transpose (including the new ones)\n",
    "metrics = ['95% Coverage', 'PI Width', 'RMSE', 'NRMSE', 'KL Divergence', 'Hellinger Distance', 'Wasserstein Distance', \n",
    "           'Training time (s)', 'Prediction time (s)']\n",
    "\n",
    "# Print each metric row with values for each method\n",
    "for metric in metrics:\n",
    "    row = f\"{metric:<30}\" + \" \".join([f\"{results_df.loc[index, metric]:<15}\" for index in range(len(methods))])\n",
    "    print(row)\n",
    "\n",
    "    if metric == 'Wasserstein Distance':\n",
    "        print('-' * len(header))\n",
    "        \n",
    "print('-' * len(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
